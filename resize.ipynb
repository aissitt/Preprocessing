{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "from pathlib import Path\n",
    "import albumentations\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"Enter the path to the model directory: \"\n",
    "# dir_path = \"C:\\\\Users\\\\--\\\\Documents\\\\uot16\"\n",
    "dir_path = \"MODEL DIR PATH\"\n",
    "folder_holding_dataset = os.path.join(dir_path, \"preprocessing\", \"dataset\")\n",
    "counter = 0\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(img_arr, bboxes, h, w):\n",
    "    \"\"\"\n",
    "    :param img_arr: original image as a numpy array\n",
    "    :param bboxes: bboxes as numpy array where each row is 'x_min', 'y_min', 'x_max', 'y_max', \"class_id\"\n",
    "    :param h: resized height dimension of image\n",
    "    :param w: resized weight dimension of image\n",
    "    :return: dictionary containing {image:transformed, bboxes:['x_min', 'y_min', 'x_max', 'y_max', \"class_id\"]}\n",
    "    \"\"\"\n",
    "    # create resize transform pipeline\n",
    "    transform = albumentations.Compose(\n",
    "        [albumentations.Resize(height=h, width=w, always_apply=True)],\n",
    "        bbox_params=albumentations.BboxParams(format='pascal_voc'))\n",
    "\n",
    "    transformed = transform(image=img_arr, bboxes=bboxes)\n",
    "    print(transformed[\"bboxes\"])\n",
    "    \n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get class names from dataset directory\n",
    "classes = [c.name for c in os.scandir(folder_holding_dataset) if c.is_dir()]\n",
    "\n",
    "for c in classes:\n",
    "  print(c) \n",
    "\n",
    "  # get path of each class directory\n",
    "  class_folder = os.path.join(folder_holding_dataset, c)\n",
    "\n",
    "  # get file names from class directory\n",
    "  files = [f.name for f in os.scandir(class_folder) if f.is_file()]\n",
    "\n",
    "  for f in files:\n",
    "    # get path of each file \n",
    "    file_path = os.path.join(class_folder, f)\n",
    "\n",
    "    # # files are named as integers (ex. 1-800)\n",
    "    # # gets integer \n",
    "    # count=int(Path(file_path).stem)\n",
    "    # print(count)\n",
    "\n",
    "    # # gets image path\n",
    "    # img_path =   os.path.join(class_folder, str(count)+\".jpg\")\n",
    "    # # gets annotation path\n",
    "    # xml_path =   os.path.join(class_folder, str(count)+\".xml\")\n",
    "\n",
    "    # gets image path\n",
    "    img_path =   os.path.join(class_folder, Path(f).stem+\".jpg\")\n",
    "    # gets annotation path\n",
    "    xml_path =   os.path.join(class_folder, Path(f).stem+\".xml\")\n",
    "\n",
    "    im = Image.open(img_path)\n",
    "    # display(im)\n",
    "\n",
    "    # create numpy array\n",
    "    im = np.asarray(im)\n",
    "\n",
    "    # gets xml info so we can modify\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    sample_annotations = []\n",
    "    # get image width and height from xml\n",
    "    for neighbor in root.iter('size'):\n",
    "      width = int(neighbor.find('width').text)\n",
    "      height = int(neighbor.find('height').text)\n",
    "\n",
    "    # get bounding box data from xml\n",
    "    for neighbor in root.iter('bndbox'):\n",
    "      # if original annotations fall outside actual image, we correct\n",
    "      # ~1 pixel area around border of image where model will detect poorly\n",
    "      xmin = max(int(float(neighbor.find('xmin').text)), 1)\n",
    "      ymin = max(int(float(neighbor.find('ymin').text)), 1)\n",
    "      xmax = min(int(float(neighbor.find('xmax').text)), int(width)-1)\n",
    "      ymax = min(int(float(neighbor.find('ymax').text)), int(height)-1)\n",
    "      \n",
    "      # this error value will help tell us if annotation values are equal/incorrect\n",
    "      allowed_error = 1.0\n",
    "      # print(count)\n",
    "      \n",
    "      # checks for the allowed error; checks if annotations need correction\n",
    "      if abs(xmin - xmax) <= allowed_error:\n",
    "        xmin = math.floor(xmin) \n",
    "        xmax = math.ceil(xmax)\n",
    "        if xmin == xmax:\n",
    "          xmin -= 1\n",
    "          xmax += 1 \n",
    "\n",
    "      # checks for the allowed error; checks if annotations need correction    \n",
    "      if abs(ymin - ymax) <= allowed_error:\n",
    "        ymin = math.floor(ymin) \n",
    "        ymax = math.ceil(ymax)\n",
    "        if ymin == ymax:\n",
    "          ymin -= 1\n",
    "          ymax += 1\n",
    "        \n",
    "      # collect bounding box and class annotations  \n",
    "      sample_annotations.append([round(xmin), round(ymin), round(xmax), round(ymax), c])\n",
    "    sample_annotations = np.asarray(sample_annotations, dtype = object)  \n",
    "    print(img_path)\n",
    "    print(sample_annotations)\n",
    "    \n",
    "    # transform the image using the sample annotations; pass desired resize arguments\n",
    "    transformed_im = resize_image(im, sample_annotations, 320, 320)\n",
    "    trans_im = Image.fromarray(transformed_im[\"image\"])\n",
    "    draw = ImageDraw.Draw(trans_im)\n",
    "\n",
    "    # to plot bounding boxes:\n",
    "    # for bbox in transformed_im[\"bboxes\"]:\n",
    "      # print(type(bbox[0:4]))\n",
    "      # draw.rectangle(list(bbox[0:4]), outline= \"red\")\n",
    "      # print(\"here\")\n",
    "\n",
    "    # print(transformed_im)\n",
    "    # display(trans_im)\n",
    "    \n",
    "    print(transformed_im[\"bboxes\"])\n",
    "\n",
    "    # write new annotations to the xml files\n",
    "    the_file = xml_path\n",
    "    tree = ET.parse(the_file)\n",
    "    root = tree.getroot()\n",
    "    bbox_idx = 0\n",
    "    for box in root.find(\"object\").findall(\"bndbox\"):\n",
    "      child_idx = 0\n",
    "      for child in box:\n",
    "        # print(child.text)\n",
    "        child.text = str(int(transformed_im[\"bboxes\"][bbox_idx][child_idx]))\n",
    "        child_idx+=1\n",
    "      bbox_idx+=1\n",
    "    tree.write(the_file)\n",
    "    trans_im.save(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('Nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27f0e5e1df9da351ef055c5ef3783a7898ea755580039e2618dc361c58d69156"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
